{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verbal-wilderness",
   "metadata": {},
   "source": [
    "# RetrieveTweets.ipynb\n",
    "\n",
    "### This notebook can be used to retrieve Tweets from the analyzed accounts using Twitter API.\n",
    "\n",
    "Author: Erik Puijk <br>\n",
    "Date  : February 5, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prospective-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitter in /home/erik/anaconda3/lib/python3.8/site-packages (1.19.3)\n",
      "Requirement already satisfied: requests in /home/erik/anaconda3/lib/python3.8/site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/erik/anaconda3/lib/python3.8/site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/erik/anaconda3/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/erik/anaconda3/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/erik/anaconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: matplotlib in /home/erik/anaconda3/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/erik/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/erik/anaconda3/lib/python3.8/site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/erik/.local/lib/python3.8/site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/erik/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/erik/anaconda3/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/erik/.local/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/erik/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/erik/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Install required packages. \"\"\"\n",
    "!pip install twitter\n",
    "!pip install requests\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hollow-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import datetime\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "suitable-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_token():\n",
    "    \"\"\" Read the Twitter API Token from a preset text file. \"\"\"\n",
    "    \n",
    "    token = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(\"source/token.txt\", 'r') as f:\n",
    "            token = f.read()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "        \n",
    "    os.environ['TOKEN'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "neutral-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_media_fields(tweets, media):\n",
    "    \"\"\" Merge media object information into corresponding Tweet JSON-object. \"\"\"\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Some Tweets contain media\n",
    "        if 'attachments' in tweet:\n",
    "            # Media types are the same for each Tweet\n",
    "            media_type = [medium['type'] for medium in media if medium['media_key'] in tweet['attachments']['media_keys']][0]\n",
    "            public_metrics = [medium['public_metrics'] for medium in media if medium['media_key'] in tweet['attachments']['media_keys'] and 'public_metrics' in medium]\n",
    "            \n",
    "            tweet.update({\"media_type\": media_type})\n",
    "            \n",
    "            if len(public_metrics) == 0:\n",
    "                tweet.update({\"media_public_metrics\": \"none\"})\n",
    "            else:\n",
    "                # Only videos contain public_metrics\n",
    "                tweet.update({\"media_public_metrics\": public_metrics})\n",
    "            \n",
    "            tweet.pop('attachments')\n",
    "        \n",
    "        # Other Tweets do not contain media\n",
    "        else:\n",
    "            tweet.update({\"media_type\": \"none\"})\n",
    "            tweet.update({\"media_public_metrics\": \"none\"})\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bridal-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_tweets(query, start_time, end_time, max_results, sleep_time):\n",
    "    \"\"\" Create requests for Twitter API to obtain Tweets matching the query and start/end times. \"\"\"\n",
    "    \n",
    "    bearer_token = os.getenv('TOKEN')\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    next_token = \"\"\n",
    "    tweets = []\n",
    "    i = 0\n",
    "    \n",
    "    # Continue until no more data is received\n",
    "    while True:\n",
    "        if i > 0:\n",
    "            query_params = {'query': query,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'max_results': max_results,\n",
    "                'expansions': 'attachments.media_keys',\n",
    "                'tweet.fields': 'public_metrics,created_at,author_id,conversation_id',\n",
    "                'media.fields': 'public_metrics',\n",
    "                'pagination_token': next_token}\n",
    "            \n",
    "            # Wait before the next request\n",
    "            time.sleep(sleep_time)\n",
    "        else:            \n",
    "            query_params = {'query': query,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'max_results': max_results,\n",
    "                'expansions': 'attachments.media_keys',\n",
    "                'tweet.fields': 'public_metrics,created_at,author_id,conversation_id',\n",
    "                'media.fields': 'public_metrics'}\n",
    "\n",
    "        response = requests.request(\"GET\", search_url, headers = headers, params = query_params)\n",
    "        \n",
    "        # Stop if error occurs\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "            break\n",
    "    \n",
    "        # Stop of no data found\n",
    "        res = response.json()\n",
    "        if 'data' not in res:\n",
    "            print(\"Data not found:\")\n",
    "            print(res)\n",
    "            break\n",
    "        else:\n",
    "            res_tweets = merge_media_fields(res['data'], res['includes']['media'])\n",
    "        \n",
    "        # Append new Tweets to Tweets list\n",
    "        for tweet in res_tweets:\n",
    "            # Remove a Tweet that is not in Dutch\n",
    "            if tweet['id'] != \"1370841025829298176\":\n",
    "                tweets.append(tweet)\n",
    "            \n",
    "        # Stop if no next token\n",
    "        res = response.json()\n",
    "        if 'next_token' not in res['meta']:\n",
    "            break\n",
    "\n",
    "        next_token = res['meta']['next_token']\n",
    "        i += 1\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "widespread-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_users():\n",
    "    \"\"\" Read user information contained in preset text file. \"\"\"\n",
    "    \n",
    "    content = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(\"source/users.txt\", 'r') as f:\n",
    "            content = json.loads(f.read())\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "apparent-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_token()\n",
    "users = read_users()\n",
    "\n",
    "start_time = \"2021-02-04T00:00:00.000Z\"\n",
    "end_time = \"2021-03-18T00:00:00.000Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "southeast-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collections(tweets):\n",
    "    \"\"\" Remove all collection Tweets (messages that do not fit in one Tweet and are split into a collection). \"\"\"\n",
    "    \n",
    "    tweets2 = []\n",
    "\n",
    "    # To find a collection, we need to temporarily remove some parts of a Tweet\n",
    "    for tweet in tweets:\n",
    "        # Remove hashtags\n",
    "        text = re.sub('#[A-Za-z0-9_]+','', tweet['text'])\n",
    "        \n",
    "        # Remove links\n",
    "        text = re.sub('http[A-Za-z0-9_:/.]+','', text)\n",
    "        \n",
    "        # Remove trailing whitespaces\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Match patterns of collections\n",
    "        m = re.search('[0-9]*/[0-9]+\\]$|[0-9]+/[0-9]*\\]$|[0-9]*/[0-9]+\\)$|[0-9]+/[0-9]*\\)$|[0-9]+/$|/[0-9]+$|[0-9]+/[0-9]+$', text)\n",
    "        \n",
    "        if not m:\n",
    "            tweets2.append(tweet)\n",
    "    \n",
    "    return tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "wound-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total_engagement(tweet, author_handle):\n",
    "    \"\"\" Calculate total engagement of a Tweet. \"\"\"\n",
    "    \n",
    "    # Add up all interactions\n",
    "    interactions = tweet['public_metrics']['retweet_count'] + \\\n",
    "                    tweet['public_metrics']['reply_count'] + \\\n",
    "                    tweet['public_metrics']['like_count'] + \\\n",
    "                    tweet['public_metrics']['quote_count']\n",
    "    \n",
    "    followers = users[author_handle]['followers']\n",
    "    \n",
    "    # Relate to nr. of followers so comparison is possible\n",
    "    return interactions / followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "shared-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    \"\"\" Prepares parameters for Twitter API request for each user. \"\"\"\n",
    "    \n",
    "    sleep_time = 1.1\n",
    "    max_results = 500\n",
    "    tweets = []\n",
    "    \n",
    "    for handle, info in users.items():\n",
    "        query = \"from:%s lang:nl -is:retweet -is:reply -is:quote\" % (handle)\n",
    "        res = request_tweets(query, start_time, end_time, max_results, sleep_time)\n",
    "        res_clean = remove_collections(res)\n",
    "        \n",
    "        # Add additional data to Tweets\n",
    "        for tweet in res_clean:\n",
    "            tweet.update({\"author_handle\": handle})\n",
    "            tweet.update({\"total_engagement\": calc_total_engagement(tweet, handle)})\n",
    "            \n",
    "            # Category data is empty for now\n",
    "            tweet.update({\"cat_con\": \"\"})\n",
    "            tweet.update({\"cat_act\": \"\"})\n",
    "        \n",
    "        print(\"%s: %s Tweets\" % (handle, len(res_clean)))\n",
    "        \n",
    "        tweets.extend(res_clean)\n",
    "        \n",
    "        # Wait before the next request\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    print(\"Total Tweets: %s\" % (len(tweets)))\n",
    "    \n",
    "    random.shuffle(tweets)\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "illegal-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VVD: 226 Tweets\n",
      "markrutte: 34 Tweets\n",
      "D66: 334 Tweets\n",
      "SigridKaag: 59 Tweets\n",
      "geertwilderspvv: 187 Tweets\n",
      "cdavandaag: 148 Tweets\n",
      "WBHoekstra: 43 Tweets\n",
      "SPnl: 218 Tweets\n",
      "MarijnissenL: 120 Tweets\n",
      "PvdA: 288 Tweets\n",
      "PloumenLilianne: 89 Tweets\n",
      "groenlinks: 297 Tweets\n",
      "jesseklaver: 98 Tweets\n",
      "fvdemocratie: 241 Tweets\n",
      "thierrybaudet: 349 Tweets\n",
      "PartijvdDieren: 337 Tweets\n",
      "estherouwehand: 48 Tweets\n",
      "christenunie: 39 Tweets\n",
      "gertjansegers: 66 Tweets\n",
      "VoltNederland: 298 Tweets\n",
      "DassenLaurens: 27 Tweets\n",
      "JuisteAntwoord: 108 Tweets\n",
      "Eerdmans: 7 Tweets\n",
      "SGPnieuws: 124 Tweets\n",
      "keesvdstaaij: 27 Tweets\n",
      "DenkNL: 86 Tweets\n",
      "F_azarkan: 73 Tweets\n",
      "50pluspartij: 87 Tweets\n",
      "LianedenHaan: 24 Tweets\n",
      "BoerBurgerB: 80 Tweets\n",
      "lientje1967: 219 Tweets\n",
      "PolitiekBIJ1: 203 Tweets\n",
      "SylvanaBIJ1: 80 Tweets\n",
      "Total Tweets: 4664\n"
     ]
    }
   ],
   "source": [
    "tweets = get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "advised-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(path):\n",
    "    \"\"\" Write obtained Tweets to a text file in JSON-format. \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(tweets, f)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "considerable-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tweets(\"source/tweets_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-hazard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
