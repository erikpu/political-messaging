{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-hometown",
   "metadata": {},
   "source": [
    "# MachineLearning.ipynb\n",
    "\n",
    "### This notebook contains the feature generation, supervised machine learning and validation for automatic classification of Tweets.\n",
    "\n",
    "Author: Erik Puijk <br>\n",
    "Date  : March 28, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noted-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /home/erik/anaconda3/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.6 in /home/erik/.local/lib/python3.8/site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (0.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organic-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, csv, math, random\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, make_scorer, cohen_kappa_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "# https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\n",
    "# https://vitalflux.com/hold-out-method-for-training-machine-learning-model/\n",
    "# https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(path):\n",
    "    \"\"\" Read the Tweets from a given text file and return in JSON-format. \"\"\"\n",
    "    \n",
    "    content = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            content = json.loads(f.read())\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "        \n",
    "    print(\"Total Tweets read: %s\\n\" % (len(content)))\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "universal-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(tweets_w, path):\n",
    "    \"\"\" Write obtained Tweets to a text file in JSON-format. \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(tweets_w, f)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mechanical-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_count(cat):\n",
    "    \"\"\" Count the occurrences of each category in a list of categories. \"\"\"\n",
    "    \n",
    "    occurrences = [[x,cat.count(x)] for x in set(cat)]\n",
    "    \n",
    "    print('Number of training data')\n",
    "    \n",
    "    for occ in occurrences:\n",
    "        print('%s: %s' % (occ[0], occ[1]))\n",
    "        \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "temporal-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_tweets(text, labels):\n",
    "    \"\"\" Upsample Tweets from categories that are less occurring to create a balanced training set. \"\"\"\n",
    "    \n",
    "    tweets = [list(a) for a in zip(text, labels)]\n",
    "    cats = list(set(labels))\n",
    "    tweets_divided = []\n",
    "    tweets_upsampled = []\n",
    "    max_len = 0\n",
    "    \n",
    "    for i, cat in enumerate(cats):\n",
    "        tweets_divided.append([tweet for tweet in tweets if tweet[1] == cat])\n",
    "        \n",
    "        l_len = len(tweets_divided[i])\n",
    "        \n",
    "        if l_len > max_len:\n",
    "            max_len = l_len\n",
    "                    \n",
    "    for i, cat in enumerate(cats):\n",
    "        if len(tweets_divided[i]) < max_len:\n",
    "            tweets_upsampled.append(resample(tweets_divided[i], replace=True, n_samples=max_len, random_state=0))\n",
    "        else:\n",
    "            tweets_upsampled.append(tweets_divided[i])\n",
    "    \n",
    "    tweets_joined =  [item for l in tweets_upsampled for item in l]\n",
    "    random.shuffle(tweets_joined)\n",
    "    unzipped = list(zip(*tweets_joined))\n",
    "    \n",
    "    return unzipped[0], unzipped[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convenient-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_test, pred):\n",
    "    \"\"\" Call an f1-score function to calculate the performance of a model configuration. \"\"\"\n",
    "    \n",
    "    score = f1_score(y_test, pred, average='micro', zero_division=0)\n",
    "    #score = cohen_kappa_score(y_test, pred)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "critical-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_score(scores):\n",
    "    \"\"\" Calculate an average score given a list of scores. \"\"\"\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for score in scores:\n",
    "        total += score\n",
    "    \n",
    "    return total / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "comparative-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(v_type, X_train, y_train, ngram_min, ngram_max, min_df, max_df, max_features):\n",
    "    \"\"\" Perform the count/tfidf vectorization on the test set and run 5-fold cross validation on the test set\n",
    "        to compare the results. \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "           \n",
    "    # Run cross-validation and calculate scores\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    for train, valid in kf.split(X_train, y_train):\n",
    "        X_train1 = [X_train[i] for i in train]\n",
    "        y_train1 = [y_train[i] for i in train]\n",
    "        X_valid1 = [X_train[i] for i in valid]\n",
    "        y_valid1 = [y_train[i] for i in valid]\n",
    "        \n",
    "        if v_type == 'count':\n",
    "            vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram_min, ngram_max), min_df=min_df, max_df=max_df, max_features=max_features)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(ngram_min, ngram_max), min_df=min_df, max_df=max_df, max_features=max_features)\n",
    "        \n",
    "        # Oversample tweets with less occurring categories to balance training set\n",
    "        X_train1, y_train1 = resample_tweets(X_train1, y_train1)\n",
    "        \n",
    "        #stats_count(list(y_train1))\n",
    "        \n",
    "        # Encode labels\n",
    "        y_train1 = Encoder.fit_transform(y_train1)\n",
    "        y_valid1 = Encoder.fit_transform(y_valid1)\n",
    "        \n",
    "        # Vectorize data\n",
    "        X_train1 = vectorizer.fit_transform(X_train1)\n",
    "        X_valid1 = vectorizer.transform(X_valid1)\n",
    "        \n",
    "        # Train classifier\n",
    "        clf = svm.SVC(kernel='linear', C=1, class_weight=None)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "            \n",
    "        # Predict and compare with test set\n",
    "        pred = clf.predict(X_valid1)\n",
    "        scores.append(f1_score(y_valid1, pred, average='micro', zero_division=0))\n",
    "       \n",
    "    # Return the configuration of the model together with the average of scores\n",
    "    return [[ngram_min, ngram_max, min_df, max_df, max_features], calc_avg_score(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "arbitrary-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_configurations(v_type, test_validation, tweets, labels):\n",
    "    \"\"\" Test different configurations for different models and compare the micro-f1 scores to select the best\n",
    "        model. \"\"\"\n",
    "    \n",
    "    # Define hyper parameters per categorization\n",
    "    if len(list(set(labels))) == 3:\n",
    "        # Content\n",
    "        ngrams = [1, 2]\n",
    "        min_dfs = [1, 2, 3, 4, 5]\n",
    "        max_dfs = [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        max_features = [None, 1000, 2000, 3000]\n",
    "    else:\n",
    "        # Activation\n",
    "        ngrams = [1, 2, 3]\n",
    "        min_dfs = [1, 2, 3, 4, 5]\n",
    "        max_dfs = [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        max_features = [None]#, 1000, 2000, 3000]\n",
    "    \n",
    "    # Split data set into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.3, random_state=0)\n",
    "    \n",
    "    #X_train, y_train = resample_train_set(X_train, y_train)\n",
    "    \n",
    "    scores = []\n",
    "    i = 0\n",
    "    i_max = (len(ngrams)**2 - math.factorial(len(ngrams)-1)) * len(min_dfs) * len(max_dfs) * len(max_features)\n",
    "    \n",
    "    # Perform hyperparameter optimization by testing every configuration with k-fold cross validation.\n",
    "    for ngram_min in ngrams:\n",
    "        for ngram_max in ngrams:\n",
    "            if ngram_min > ngram_max:\n",
    "                continue\n",
    "            for min_df in min_dfs:\n",
    "                for max_df in max_dfs:\n",
    "                    for max_feature in max_features:\n",
    "                        i += 1\n",
    "                        print('Progress: %s/%s configurations' % (i, i_max), end='\\r')\n",
    "                        \n",
    "                        # Call the function for the model and append the score\n",
    "                        scores.append(cross_val(v_type, X_train, y_train, ngram_min, ngram_max, min_df, max_df, max_feature))\n",
    "    \n",
    "    # Sort the scores descending and print them\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if test_validation:\n",
    "        # Select the best configurations\n",
    "        best_score = scores[0][1]\n",
    "        best_configs = []\n",
    "        for config in scores:\n",
    "            if config[1] == best_score:\n",
    "                best_configs.append(config[0])\n",
    "\n",
    "        X_train, y_train = resample_tweets(X_train, y_train)\n",
    "        \n",
    "        # Encode labels\n",
    "        y_train = Encoder.fit_transform(y_train)\n",
    "        y_test = Encoder.fit_transform(y_test)\n",
    "        \n",
    "        # Use the best configurations to validate model on the test set\n",
    "        for config in best_configs:\n",
    "            print(config)\n",
    "            \n",
    "            # Select appropriate vectorizer\n",
    "            if v_type == 'count':\n",
    "                vectorizer = CountVectorizer(analyzer='word', ngram_range=(config[0], config[1]), min_df=config[2], max_df=config[3], max_features=config[4])\n",
    "            else:\n",
    "                vectorizer = TfidfVectorizer(ngram_range=(config[0], config[1]), min_df=config[2], max_df=config[3], max_features=config[4])\n",
    "    \n",
    "            # Train with vectorized features\n",
    "            X_train_t = vectorizer.fit_transform(X_train)\n",
    "            X_test_t = vectorizer.transform(X_test)\n",
    "            clf = svm.SVC(kernel='linear', C=1, class_weight=None)\n",
    "            clf.fit(X_train_t, y_train)\n",
    "            \n",
    "            # Predict and compare with test set\n",
    "            pred = clf.predict(X_test_t)\n",
    "            print(classification_report(y_test, pred, target_names=Encoder.inverse_transform(list(set(y_test))), zero_division=0))\n",
    "\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hydraulic-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets read: 4664\n",
      "\n",
      "[1, 1, 4, 0.6, None]nfigurations\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CON       1.00      0.17      0.29         6\n",
      "         FOL       0.68      0.48      0.57        27\n",
      "        NONE       0.80      0.89      0.84        93\n",
      "         SUP       0.50      0.57      0.53        14\n",
      "\n",
      "    accuracy                           0.75       140\n",
      "   macro avg       0.75      0.53      0.56       140\n",
      "weighted avg       0.75      0.75      0.73       140\n",
      "\n",
      "[1, 1, 4, 0.8, None]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CON       1.00      0.17      0.29         6\n",
      "         FOL       0.68      0.48      0.57        27\n",
      "        NONE       0.80      0.89      0.84        93\n",
      "         SUP       0.50      0.57      0.53        14\n",
      "\n",
      "    accuracy                           0.75       140\n",
      "   macro avg       0.75      0.53      0.56       140\n",
      "weighted avg       0.75      0.75      0.73       140\n",
      "\n",
      "[1, 1, 4, 1.0, None]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CON       1.00      0.17      0.29         6\n",
      "         FOL       0.68      0.48      0.57        27\n",
      "        NONE       0.80      0.89      0.84        93\n",
      "         SUP       0.50      0.57      0.53        14\n",
      "\n",
      "    accuracy                           0.75       140\n",
      "   macro avg       0.75      0.53      0.56       140\n",
      "weighted avg       0.75      0.75      0.73       140\n",
      "\n",
      "[[[1, 1, 4, 0.6, None], 0.7787412587412588], [[1, 1, 4, 0.8, None], 0.7787412587412588], [[1, 1, 4, 1.0, None], 0.7787412587412588], [[1, 2, 3, 0.4, None], 0.7726806526806527], [[1, 2, 3, 0.6, None], 0.7725874125874126], [[1, 2, 3, 0.8, None], 0.7725874125874126], [[1, 2, 3, 1.0, None], 0.7725874125874126], [[1, 2, 5, 0.6, None], 0.7725874125874126], [[1, 2, 5, 0.8, None], 0.7725874125874126], [[1, 2, 5, 1.0, None], 0.7725874125874126], [[1, 1, 3, 0.6, None], 0.7696037296037297], [[1, 1, 3, 0.8, None], 0.7696037296037297], [[1, 1, 3, 1.0, None], 0.7696037296037297], [[1, 1, 4, 0.4, None], 0.7695104895104896], [[1, 1, 5, 0.6, None], 0.7695104895104896], [[1, 1, 5, 0.8, None], 0.7695104895104896], [[1, 1, 5, 1.0, None], 0.7695104895104896], [[1, 1, 3, 0.4, None], 0.7664801864801865], [[1, 2, 3, 0.2, None], 0.7634965034965034], [[1, 1, 2, 0.6, None], 0.7634498834498834], [[1, 1, 2, 0.8, None], 0.7634498834498834], [[1, 1, 2, 1.0, None], 0.7634498834498834], [[1, 3, 5, 0.6, None], 0.7634032634032634], [[1, 3, 5, 0.8, None], 0.7634032634032634], [[1, 3, 5, 1.0, None], 0.7634032634032634], [[1, 1, 1, 0.4, None], 0.7603729603729603], [[1, 2, 2, 0.8, None], 0.7603729603729603], [[1, 1, 5, 0.4, None], 0.7603263403263403], [[1, 2, 2, 0.6, None], 0.7573426573426574], [[1, 2, 2, 1.0, None], 0.7573426573426574], [[1, 1, 1, 0.6, None], 0.7572960372960372], [[1, 1, 1, 0.8, None], 0.7572960372960372], [[1, 1, 1, 1.0, None], 0.7572960372960372], [[1, 2, 4, 0.4, None], 0.7572494172494173], [[1, 2, 4, 0.6, None], 0.7572494172494173], [[1, 2, 4, 0.8, None], 0.7572494172494173], [[1, 2, 4, 1.0, None], 0.7572494172494173], [[1, 2, 5, 0.4, None], 0.7572494172494173], [[1, 3, 3, 0.6, None], 0.7542657342657343], [[1, 3, 3, 0.8, None], 0.7542657342657343], [[1, 3, 3, 1.0, None], 0.7542657342657343], [[1, 3, 4, 0.6, None], 0.7541724941724942], [[1, 3, 4, 0.8, None], 0.7541724941724942], [[1, 3, 4, 1.0, None], 0.7541724941724942], [[1, 2, 3, 0.1, None], 0.7513752913752914], [[1, 1, 2, 0.4, None], 0.7511888111888112], [[1, 3, 3, 0.4, None], 0.7511888111888111], [[1, 3, 5, 0.4, None], 0.7510955710955711], [[1, 2, 5, 0.2, None], 0.7482051282051282], [[1, 3, 3, 0.1, None], 0.7482051282051282], [[1, 1, 3, 0.2, None], 0.7481118881118882], [[1, 2, 4, 0.2, None], 0.7481118881118881], [[1, 3, 4, 0.4, None], 0.7480186480186479], [[1, 2, 2, 0.2, None], 0.7451748251748251], [[1, 2, 2, 0.4, None], 0.7451282051282051], [[1, 1, 1, 0.2, None], 0.7450815850815851], [[1, 3, 3, 0.2, None], 0.7450815850815851], [[1, 2, 4, 0.1, None], 0.7420512820512821], [[1, 3, 4, 0.2, None], 0.742004662004662], [[1, 2, 2, 0.1, None], 0.739020979020979], [[1, 3, 2, 0.4, None], 0.738974358974359], [[1, 3, 4, 0.1, None], 0.738974358974359], [[1, 3, 5, 0.2, None], 0.7389277389277389], [[1, 3, 2, 0.2, None], 0.735990675990676], [[1, 1, 4, 0.2, None], 0.7359440559440559], [[1, 2, 1, 0.6, None], 0.735897435897436], [[1, 2, 1, 0.8, None], 0.735897435897436], [[1, 2, 1, 1.0, None], 0.735897435897436], [[1, 3, 2, 0.6, None], 0.7358974358974358], [[1, 3, 2, 0.8, None], 0.7358974358974358], [[1, 3, 2, 1.0, None], 0.7358974358974358], [[1, 3, 5, 0.1, None], 0.7329137529137529], [[1, 2, 1, 0.4, None], 0.7328205128205127], [[1, 1, 2, 0.2, None], 0.7297902097902098], [[1, 3, 1, 0.6, None], 0.7267599067599066], [[1, 3, 1, 0.8, None], 0.7267599067599066], [[1, 1, 5, 0.2, None], 0.7267132867132867], [[1, 3, 1, 0.4, None], 0.7267132867132867], [[1, 2, 1, 0.2, None], 0.7236829836829837], [[1, 3, 1, 1.0, None], 0.7236829836829837], [[1, 3, 2, 0.1, None], 0.7236829836829837], [[1, 2, 5, 0.1, None], 0.7236829836829836], [[1, 1, 3, 0.1, None], 0.7175291375291376], [[1, 1, 2, 0.1, None], 0.7174825174825175], [[1, 2, 1, 0.1, None], 0.7174358974358974], [[1, 3, 1, 0.2, None], 0.7144522144522145], [[1, 2, 2, 0.05, None], 0.7082983682983682], [[1, 1, 1, 0.1, None], 0.7082517482517482], [[1, 1, 4, 0.1, None], 0.7054079254079254], [[1, 3, 1, 0.1, None], 0.7022843822843823], [[1, 2, 3, 0.05, None], 0.7021911421911422], [[1, 3, 2, 0.05, None], 0.7021911421911422], [[2, 2, 5, 0.1, None], 0.6931934731934734], [[1, 1, 5, 0.1, None], 0.693100233100233], [[1, 2, 1, 0.05, None], 0.692960372960373], [[1, 3, 3, 0.05, None], 0.6929137529137529], [[2, 2, 4, 0.1, None], 0.6901165501165503], [[2, 2, 4, 0.2, None], 0.6901165501165503], [[2, 2, 4, 0.4, None], 0.6901165501165503], [[2, 2, 4, 0.6, None], 0.6901165501165503], [[2, 2, 4, 0.8, None], 0.6901165501165503], [[2, 2, 4, 1.0, None], 0.6901165501165503], [[2, 2, 5, 0.2, None], 0.6901165501165503], [[2, 2, 5, 0.4, None], 0.6901165501165503], [[2, 2, 5, 0.6, None], 0.6901165501165503], [[2, 2, 5, 0.8, None], 0.6901165501165503], [[2, 2, 5, 1.0, None], 0.6901165501165503], [[2, 2, 1, 0.1, None], 0.684055944055944], [[2, 2, 1, 0.2, None], 0.684055944055944], [[2, 2, 1, 0.4, None], 0.684055944055944], [[2, 2, 1, 0.6, None], 0.684055944055944], [[2, 2, 1, 0.8, None], 0.684055944055944], [[2, 2, 1, 1.0, None], 0.684055944055944], [[2, 3, 4, 0.1, None], 0.6840093240093241], [[2, 3, 4, 0.2, None], 0.6840093240093241], [[2, 3, 4, 0.4, None], 0.6840093240093241], [[2, 3, 4, 0.6, None], 0.6840093240093241], [[2, 3, 4, 0.8, None], 0.6840093240093241], [[2, 3, 4, 1.0, None], 0.6840093240093241], [[2, 3, 5, 0.1, None], 0.6840093240093241], [[2, 3, 5, 0.2, None], 0.6840093240093241], [[2, 3, 5, 0.4, None], 0.6840093240093241], [[2, 3, 5, 0.6, None], 0.6840093240093241], [[2, 3, 5, 0.8, None], 0.6840093240093241], [[2, 3, 5, 1.0, None], 0.6840093240093241], [[2, 2, 2, 0.1, None], 0.6808857808857808], [[2, 2, 2, 0.2, None], 0.6808857808857808], [[2, 2, 2, 0.4, None], 0.6808857808857808], [[2, 2, 2, 0.6, None], 0.6808857808857808], [[2, 2, 2, 0.8, None], 0.6808857808857808], [[2, 2, 2, 1.0, None], 0.6808857808857808], [[1, 1, 1, 0.05, None], 0.6806993006993007], [[1, 3, 4, 0.05, None], 0.6776689976689978], [[1, 2, 4, 0.05, None], 0.6746386946386946], [[1, 3, 1, 0.05, None], 0.6745920745920746], [[2, 2, 3, 0.1, None], 0.6716550116550116], [[2, 2, 3, 0.2, None], 0.6716550116550116], [[2, 2, 3, 0.4, None], 0.6716550116550116], [[2, 2, 3, 0.6, None], 0.6716550116550116], [[2, 2, 3, 0.8, None], 0.6716550116550116], [[2, 2, 3, 1.0, None], 0.6716550116550116], [[2, 2, 5, 0.05, None], 0.6687179487179487], [[1, 1, 2, 0.05, None], 0.6684848484848485], [[1, 2, 5, 0.05, None], 0.6683916083916084], [[2, 2, 4, 0.05, None], 0.6625174825174825], [[1, 1, 3, 0.05, None], 0.6593473193473194], [[1, 3, 5, 0.05, None], 0.6592074592074593], [[2, 3, 2, 0.1, None], 0.6564102564102565], [[2, 3, 2, 0.2, None], 0.6564102564102565], [[2, 3, 2, 0.4, None], 0.6564102564102565], [[2, 3, 2, 0.6, None], 0.6564102564102565], [[2, 3, 2, 0.8, None], 0.6564102564102565], [[2, 3, 2, 1.0, None], 0.6564102564102565], [[2, 3, 3, 0.1, None], 0.6564102564102564], [[2, 3, 3, 0.2, None], 0.6564102564102564], [[2, 3, 3, 0.4, None], 0.6564102564102564], [[2, 3, 3, 0.6, None], 0.6564102564102564], [[2, 3, 3, 0.8, None], 0.6564102564102564], [[2, 3, 3, 1.0, None], 0.6564102564102564], [[2, 2, 2, 0.05, None], 0.6563636363636363], [[2, 3, 1, 0.1, None], 0.6534265734265734], [[2, 3, 1, 0.2, None], 0.6534265734265734], [[2, 3, 1, 0.4, None], 0.6534265734265734], [[2, 3, 1, 0.6, None], 0.6534265734265734], [[2, 3, 1, 0.8, None], 0.6534265734265734], [[2, 3, 1, 1.0, None], 0.6534265734265734], [[2, 3, 5, 0.05, None], 0.6534265734265734], [[2, 2, 1, 0.05, None], 0.6533799533799535], [[1, 1, 4, 0.05, None], 0.6532400932400932], [[2, 3, 4, 0.05, None], 0.6502564102564102], [[2, 2, 3, 0.05, None], 0.6502097902097901], [[1, 1, 5, 0.05, None], 0.6408391608391608], [[2, 3, 1, 0.05, None], 0.6350116550116549], [[2, 3, 2, 0.05, None], 0.6349650349650349], [[3, 3, 1, 0.1, None], 0.6258275058275058], [[3, 3, 1, 0.2, None], 0.6258275058275058], [[3, 3, 1, 0.4, None], 0.6258275058275058], [[3, 3, 1, 0.6, None], 0.6258275058275058], [[3, 3, 1, 0.8, None], 0.6258275058275058], [[3, 3, 1, 1.0, None], 0.6258275058275058], [[3, 3, 2, 0.1, None], 0.6257808857808858], [[3, 3, 2, 0.2, None], 0.6257808857808858], [[3, 3, 2, 0.4, None], 0.6257808857808858], [[3, 3, 2, 0.6, None], 0.6257808857808858], [[3, 3, 2, 0.8, None], 0.6257808857808858], [[3, 3, 2, 1.0, None], 0.6257808857808858], [[3, 3, 3, 0.1, None], 0.6257808857808858], [[3, 3, 3, 0.2, None], 0.6257808857808858], [[3, 3, 3, 0.4, None], 0.6257808857808858], [[3, 3, 3, 0.6, None], 0.6257808857808858], [[3, 3, 3, 0.8, None], 0.6257808857808858], [[3, 3, 3, 1.0, None], 0.6257808857808858], [[3, 3, 4, 0.1, None], 0.6257808857808858], [[3, 3, 4, 0.2, None], 0.6257808857808858], [[3, 3, 4, 0.4, None], 0.6257808857808858], [[3, 3, 4, 0.6, None], 0.6257808857808858], [[3, 3, 4, 0.8, None], 0.6257808857808858], [[3, 3, 4, 1.0, None], 0.6257808857808858], [[2, 3, 3, 0.05, None], 0.6256876456876457], [[3, 3, 4, 0.05, None], 0.6227505827505827], [[3, 3, 1, 0.05, None], 0.6197202797202797], [[3, 3, 2, 0.05, None], 0.6196736596736596], [[3, 3, 3, 0.05, None], 0.6196736596736596], [[3, 3, 5, 0.1, None], 0.6196270396270396], [[3, 3, 5, 0.2, None], 0.6196270396270396], [[3, 3, 5, 0.4, None], 0.6196270396270396], [[3, 3, 5, 0.6, None], 0.6196270396270396], [[3, 3, 5, 0.8, None], 0.6196270396270396], [[3, 3, 5, 1.0, None], 0.6196270396270396], [[3, 3, 5, 0.05, None], 0.6165967365967366]]\n"
     ]
    }
   ],
   "source": [
    "tweets_all = read_tweets('source/tweets_all_preprocessed.txt')\n",
    "tweets_gs = [tweet for tweet in tweets_all if tweet['memo'] == 'gold_standard']\n",
    "\n",
    "tweets_text = [tweet['text'] for tweet in tweets_gs]\n",
    "\n",
    "labels_con = [tweet['cat_con'] for tweet in tweets_gs]\n",
    "labels_act = [tweet['cat_act'] for tweet in tweets_gs]\n",
    "\n",
    "#test_configurations('tfidf', True, tweets_text, labels_con)\n",
    "test_configurations('tfidf', True, tweets_text, labels_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-gibson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
