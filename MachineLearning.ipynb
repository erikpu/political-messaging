{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-hometown",
   "metadata": {},
   "source": [
    "# MachineLearning.ipynb\n",
    "\n",
    "### This notebook contains the feature generation, supervised machine learning and validation for automatic classification of Tweets.\n",
    "\n",
    "Author: Erik Puijk <br>\n",
    "Date  : March 28, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "noted-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.7 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.6 in /home/erik/.local/lib/python3.8/site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/erik/anaconda3/lib/python3.8/site-packages (from scikit-learn) (0.17.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "Successfully installed scikit-learn-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "organic-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "basic-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "# https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\n",
    "# https://vitalflux.com/hold-out-method-for-training-machine-learning-model/\n",
    "# https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "killing-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(path):\n",
    "    \"\"\" Read the Tweets from a given text file and return in JSON-format. \"\"\"\n",
    "    \n",
    "    content = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            content = json.loads(f.read())\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "        \n",
    "    print(\"Total Tweets read: %s\\n\" % (len(content)))\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "universal-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(tweets_w, path):\n",
    "    \"\"\" Write obtained Tweets to a text file in JSON-format. \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(tweets_w, f)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "mechanical-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_count(cat):\n",
    "    \n",
    "    print('POL: %s | %s' % (round((cat.count('POL') / len(cat) * 100)), 67))\n",
    "    print('CAM: %s | %s' % (round((cat.count('CAM') / len(cat) * 100)), 26))\n",
    "    print('SOC: %s | %s' % (round((cat.count('SOC') / len(cat) * 100)), 7))\n",
    "    print('')\n",
    "    print('NONE: %s | %s' % (round((cat.count('NONE') / len(cat) * 100)), 63))\n",
    "    print('SUP: %s | %s' % (round((cat.count('SUP') / len(cat) * 100)), 15))\n",
    "    print('CON: %s | %s' % (round((cat.count('CON') / len(cat) * 100)), 3))\n",
    "    print('FOL: %s | %s' % (round((cat.count('FOL') / len(cat) * 100)), 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "arbitrary-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tweets, cat):\n",
    "    \n",
    "    # Split data set into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tweets, cat, test_size=0.3, random_state=0)\n",
    "        \n",
    "    # Encode labels\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(y_train)\n",
    "    y_test = Encoder.fit_transform(y_test)\n",
    "    \n",
    "    Tfidf_vect = TfidfVectorizer(max_features=3000)\n",
    "    Tfidf_vect.fit(tweets)\n",
    "    \n",
    "    X_train = Tfidf_vect.transform(X_train)\n",
    "    X_test = Tfidf_vect.transform(X_test)\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    scores = metrics.f1_score(y_test, pred, average='micro')\n",
    "    #scores = clf.score(X_test, y_test)\n",
    "\n",
    "    print(scores)\n",
    "    \n",
    "    #SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    #SVM.fit(X_train, y_train)\n",
    "    #predictions_SVM = SVM.predict(X_test)\n",
    "    #print(*list(zip(Encoder.inverse_transform(y_test), Encoder.inverse_transform(predictions_SVM))), sep='\\n')\n",
    "    #print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "hydraulic-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets read: 4664\n",
      "\n",
      "0.8142857142857143\n",
      "0.7285714285714285\n"
     ]
    }
   ],
   "source": [
    "tweets_r = read_tweets('source/tweets_all_preprocessed.txt')\n",
    "\n",
    "tweets_tokens = [tweet_r['text'].split() for tweet_r in tweets_r if tweet_r['memo'] == 'gold_standard']\n",
    "tweets = [tweet_r['text'] for tweet_r in tweets_r if tweet_r['memo'] == 'gold_standard']\n",
    "cat_con = [tweet_r['cat_con'] for tweet_r in tweets_r if tweet_r['memo'] == 'gold_standard']\n",
    "cat_act = [tweet_r['cat_act'] for tweet_r in tweets_r if tweet_r['memo'] == 'gold_standard']\n",
    "\n",
    "classify(tweets, cat_con)\n",
    "classify(tweets, cat_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stainless-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
