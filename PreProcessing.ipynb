{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-hometown",
   "metadata": {},
   "source": [
    "# PreProcessing.ipynb\n",
    "\n",
    "### This notebook pre-processes the Tweets for use in the SVM model.\n",
    "\n",
    "Author: Erik Puijk <br>\n",
    "Date  : February 17, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "productive-mauritius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/erik/anaconda3/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: nltk in /home/erik/anaconda3/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /home/erik/anaconda3/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in /home/erik/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/erik/anaconda3/lib/python3.8/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /home/erik/anaconda3/lib/python3.8/site-packages (from nltk) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Install required packages. \"\"\"\n",
    "!pip install unidecode\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organic-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pyperclip as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Download and load stop words list. \"\"\"\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('dutch')\n",
    "\n",
    "# Some stop words are interesting because they refer to an individual or a group\n",
    "exceptions = ['mij', 'mijn', 'ik', 'jij', 'je', 'u', 'uw', 'ons', 'onze', 'hij', 'hem', 'zij', 'haar', 'we', 'wij', 'me']\n",
    "stop_words = [word for word in stop_words if word not in exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "killing-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(path):\n",
    "    \"\"\" Read the Tweets from a given text file and return in JSON-format. \"\"\"\n",
    "    \n",
    "    content = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            content = json.loads(f.read())\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "        \n",
    "    print(\"Total Tweets read: %s\" % (len(content)))\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "critical-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_processing(tweet):\n",
    "    \"\"\" Process a Tweet by removing or replacing certain elements using regular expressions. \"\"\"\n",
    "    \n",
    "    # To lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub('http[a-z0-9_:/.]+','', tweet)\n",
    "    \n",
    "    # Replace @mentions with 'NAME'\n",
    "    tweet = re.sub(r'@([a-z0-9_]+)', ' NAME', tweet)\n",
    "    \n",
    "    # Replace times with 'TIME'\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2} tot [\\d]{1,2}\\b)', 'TIME tot TIME', tweet)\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2}[:\\.][\\d]{1,2} (uur|u)\\b)', 'TIME', tweet)\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2}[:\\.][\\d]{1,2}\\b)', 'TIME', tweet)\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2} (uur|u)\\b)', 'TIME', tweet)\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2}(uur|u)\\b)', 'TIME', tweet)\n",
    "    \n",
    "    # Replace days with 'DAY'\n",
    "    tweet = re.sub(r'\\b(maandag|dinsdag|woensdag|donderdag|vrijdag|zaterdag|zondag)+(ochtend|morgen|middag|avond)*\\b', 'DAY', tweet)\n",
    "    \n",
    "    # Replace dates with 'DATE'\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2}\\s(januari|februari|maart|april|juni|juli|augustus|september|oktober|november|december))', 'DATE', tweet)\n",
    "    tweet = re.sub(r'(\\b[\\d]{1,2}\\s(jan|feb|mrt|apr|mei|jun|jul|aug|sept|okt|nov|dec))', 'DATE', tweet)\n",
    "\n",
    "    # Replace numbers with 'NUMBER'\n",
    "    tweet = re.sub(r'\\b[\\d]+\\b', 'NUMBER', tweet)\n",
    "    \n",
    "    # Remove HTML codes\n",
    "    tweet = re.sub(r'&gt', '', tweet)\n",
    "    tweet = re.sub(r'&lt', '', tweet)\n",
    "    tweet = re.sub(r'&amp', '', tweet)\n",
    "    \n",
    "    # Replace contractions?\n",
    "    # Remove hashtags?\n",
    "    \n",
    "    # Return only alphanumerical characters and hashtags (#)\n",
    "    return re.sub(r'[^\\w#]', ' ', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adult-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tweet):\n",
    "    \"\"\" Remove stop words from a Tweet using NLTK set of Dutch stop words. \"\"\"\n",
    "    \n",
    "    filtered_tweet = ' '.join([w for w in tweet.split() if not w in stop_words])\n",
    "    \n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "divine-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(tweets):\n",
    "    \"\"\" Pre-process the list of Tweets by performing several separate manipulations. \"\"\"\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet['text'] = regex_processing(tweet['text'])\n",
    "        \n",
    "        # Remove words with one letter\n",
    "        tweet['text'] = ' '.join([w for w in tweet['text'].split() if len(w)>1])\n",
    "        \n",
    "        # Replace accents\n",
    "        tweet['text'] = unidecode.unidecode(tweet['text'])\n",
    "        \n",
    "        tweet['text'] = remove_stop_words(tweet['text'])\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unnecessary-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(tweets_w, path):\n",
    "    \"\"\" Write obtained Tweets to a text file in JSON-format. \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(tweets_w, f)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "express-array",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets read: 4664\n"
     ]
    }
   ],
   "source": [
    "tweets = read_tweets('source/tweets_all.txt')\n",
    "tweets = pre_process(tweets)\n",
    "\n",
    "write_tweets(tweets, 'source/tweets_all_preprocessed_exc_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-skirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-qatar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
